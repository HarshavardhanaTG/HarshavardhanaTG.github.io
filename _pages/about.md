---
layout: about
title: About
permalink: /
subtitle: Building accessible SPEECH and LANGUAGE technologies

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---


My name is Harsha. I am a graduate student in the ECE department at the University of California, Davis. I develop `accessible technologies` with focus on `language and speech` modalities.

I use EMG, MEG, and EEG signals to develop `fluid neuromuscular` :ocean: :brain: :muscle: interfaces that let users interact with computers in a natural way using hand gestures, handwriting, and speech. I am developing a wristband-based interface for handwriting and hand gesture decoding and an orofacial-sensor interface for speech articulation decoding using surface EMG.

Broadly, I am interested in solving two problems. First, I want to architect neural networks that are tiny and can be trained with little data. Second, I want to develop zero-shot or few-shot learning paradigms to adapt to the idiosyncrasies, anatomy, physiology, and neural drive characteristics of different individuals.

Our team at UC Davis has collected surface EMG data from hundreds of human subjects as they engage in producing hand gestures, hand writing, and speech articulation. We have open-sourced all the data we have collected, along with the corresponding code, to ensure transparency and reproducibility. Check out my [blog](https://HarshavardhanaTG.github.io/blog/) and [publications](https://HarshavardhanaTG.github.io/publications/) for more details. 

My research focuses on understanding the theoretical aspects of multivariate timeseries given by EMG, MEG, and EEG signals. Data sampled at spatially separated channels in these signals are highly correlated and are defined by the underlying functional connectivity of the neural and neuromuscular systems. Hence, these multivariate signals exhibit non-Euclidean data structure defined on a cone manifold. Unlike images, language, and audio which are sampled on a grid structure and whose statistical properties can be captured using convolutional filters defined in the Euclidean domain, these signals need filters defined in the manifold space. For detailed mathematical description, read my [blog](https://HarshavardhanaTG.github.io/blog/). To learn efficient `abstractions` of data, I use principles from `geometric machine learning` and `graph neural networks`.

I have a Bachelor's degree in Avionics from Indian Institute of Space Science and Technology. Previously, I worked at the Indian Space Research Organization :rocket: and built satellites :satellite:.